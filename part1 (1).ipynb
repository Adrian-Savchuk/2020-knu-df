{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "part1",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAY1VTg5x9qq",
        "colab_type": "text"
      },
      "source": [
        "# Model definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Madm9Eoix9qs",
        "colab_type": "text"
      },
      "source": [
        "Below is an implementation of the [LeNet](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html) architecture for the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-3hUt5fx9qv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xP8oh2X2x9q7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LeNet Model definition\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om3arARpx9rD",
        "colab_type": "code",
        "outputId": "0f4a02a8-bdb6-481d-98e4-b551fc9423c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Define what device we are using\n",
        "print(\"CUDA Available: \",torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Available:  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCsFe_hUx9rK",
        "colab_type": "text"
      },
      "source": [
        "Of note: This notebook uses only a single GPU.\n",
        "PyTorch can run models on several GPU, try to search how to specify several GPUs and create several devices.\n",
        "model = Net().to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b92e0758-195f-4673-f9cc-99bc71677d16",
        "id": "VS8lj7EyhAzX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "model = Net().to(device)\n",
        "model"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
              "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
              "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kpNVQWvx9ro",
        "colab_type": "text"
      },
      "source": [
        "## Master way"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5Uhkn9Lx9rq",
        "colab_type": "text"
      },
      "source": [
        "Or inspect code for training a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bfUgVkYx9rs",
        "colab_type": "code",
        "outputId": "aecc9dff-e6d2-4fd6-cc46-9dbae2d14a05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('./mnist', \n",
        "                   download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                   ])),\n",
        "    batch_size=128, shuffle=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:01, 8089202.28it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./mnist/MNIST/raw/train-images-idx3-ubyte.gz to ./mnist/MNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/28881 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 129902.16it/s]           \n",
            "  0%|          | 0/1648877 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./mnist/MNIST/raw/train-labels-idx1-ubyte.gz to ./mnist/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 2140331.55it/s]                            \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to ./mnist/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 49840.13it/s]            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./mnist/MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yGfvkGBx9rz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Use Stochastic Gradient Descent\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_5F2MpXx9r8",
        "colab_type": "code",
        "outputId": "0f12e881-327e-4585-b03d-7e36734361d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "epoch_num = 20\n",
        "for epoch in range(epoch_num):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Remember a line with model.to(device)?\n",
        "        # It moves a model to a GPU and PyTorch expects that\n",
        "        # input data also will be on the GPU where the model resides\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        \n",
        "        # Calculate the error between model predictins and actual labels\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Initiate backward propagation\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:    # print every 200 mini-batches\n",
        "            print(f'[Epoch {epoch + 1}: batch {i + 1}] loss: {running_loss / 200}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 1: batch 200] loss: 2.291912612915039\n",
            "[Epoch 1: batch 400] loss: 2.2420103108882903\n",
            "[Epoch 2: batch 200] loss: 1.7662313979864122\n",
            "[Epoch 2: batch 400] loss: 1.1717280262708665\n",
            "[Epoch 3: batch 200] loss: 0.8511688721179962\n",
            "[Epoch 3: batch 400] loss: 0.742559602111578\n",
            "[Epoch 4: batch 200] loss: 0.6571808513998986\n",
            "[Epoch 4: batch 400] loss: 0.6158310669660568\n",
            "[Epoch 5: batch 200] loss: 0.5636150392889977\n",
            "[Epoch 5: batch 400] loss: 0.5445651832222939\n",
            "[Epoch 6: batch 200] loss: 0.5125834642350674\n",
            "[Epoch 6: batch 400] loss: 0.4894846361875534\n",
            "[Epoch 7: batch 200] loss: 0.4771484616398811\n",
            "[Epoch 7: batch 400] loss: 0.45511133134365084\n",
            "[Epoch 8: batch 200] loss: 0.4432361833006144\n",
            "[Epoch 8: batch 400] loss: 0.42647586211562155\n",
            "[Epoch 9: batch 200] loss: 0.4308468564599752\n",
            "[Epoch 9: batch 400] loss: 0.4117084227502346\n",
            "[Epoch 10: batch 200] loss: 0.391351705789566\n",
            "[Epoch 10: batch 400] loss: 0.386165821775794\n",
            "[Epoch 11: batch 200] loss: 0.3767972456663847\n",
            "[Epoch 11: batch 400] loss: 0.3698114259541035\n",
            "[Epoch 12: batch 200] loss: 0.36606081418693065\n",
            "[Epoch 12: batch 400] loss: 0.35266258880496026\n",
            "[Epoch 13: batch 200] loss: 0.3456528715789318\n",
            "[Epoch 13: batch 400] loss: 0.3471254450082779\n",
            "[Epoch 14: batch 200] loss: 0.34317454911768436\n",
            "[Epoch 14: batch 400] loss: 0.33711898609995844\n",
            "[Epoch 15: batch 200] loss: 0.32389403492212293\n",
            "[Epoch 15: batch 400] loss: 0.323255335316062\n",
            "[Epoch 16: batch 200] loss: 0.30937296606600284\n",
            "[Epoch 16: batch 400] loss: 0.3233525429666042\n",
            "[Epoch 17: batch 200] loss: 0.3089527175575495\n",
            "[Epoch 17: batch 400] loss: 0.3005810629576445\n",
            "[Epoch 18: batch 200] loss: 0.29872506856918335\n",
            "[Epoch 18: batch 400] loss: 0.3057789145410061\n",
            "[Epoch 19: batch 200] loss: 0.28736463584005834\n",
            "[Epoch 19: batch 400] loss: 0.29573206037282945\n",
            "[Epoch 20: batch 200] loss: 0.2786044681817293\n",
            "[Epoch 20: batch 400] loss: 0.28706443376839164\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKi9iapjx9sF",
        "colab_type": "text"
      },
      "source": [
        "Let's check how accurate is our network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ypy0EeMex9sH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_test = datasets.MNIST('./mnist',\n",
        "                           train=False,\n",
        "                           download=True,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.ToTensor(),\n",
        "                           ]))\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(data_test, batch_size=256)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC2576cDx9sO",
        "colab_type": "code",
        "outputId": "15eb7fdf-cfe0-46fd-fc61-4b77d651d2be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Prevent training\n",
        "model.eval()\n",
        "\n",
        "total_correct = 0\n",
        "avg_loss = 0.0\n",
        "\n",
        "for i, data in enumerate(test_loader, 0):\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "   \n",
        "    output = model(inputs)\n",
        "    avg_loss += criterion(output, labels).sum()\n",
        "    \n",
        "    # tensor.detach() creates a tensor that shares storage with tensor that does not require grad.\n",
        "    # It detaches the output from the computational graph.\n",
        "    # So no gradient will be backpropagated along this variable.\n",
        "    pred = output.detach().max(1)[1]\n",
        "    total_correct += pred.eq(labels.view_as(pred)).sum()\n",
        "\n",
        "avg_loss /= len(data_test)\n",
        "avg_loss = avg_loss.detach().cpu().item()\n",
        "accuracy = float(total_correct) / len(data_test)\n",
        "print(f'Test Avg. Loss: {avg_loss}, Accuracy: {accuracy * 100}%')\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Avg. Loss: 0.0004083073581568897, Accuracy: 96.71%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wqse8UZex9sU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save model state for re-use\n",
        "my_awesome_model = 'my-lenet.pth'\n",
        "torch.save(model.state_dict(), my_awesome_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPLpykihx9se",
        "colab_type": "text"
      },
      "source": [
        "**End of training code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDOpLtDTx9sj",
        "colab_type": "text"
      },
      "source": [
        "**FGSM Attack**\n",
        "The fgsm_attack function takes three inputs: image is the original clean image (x), epsilon is the pixel-wise perturbation amount (ϵ), and data_grad is gradient of the loss w.r.t the input image.\n",
        "\n",
        "The function then creates perturbed image as\n",
        "\n",
        "$$\\text{perturbed image} = image + epsilon * sign(gradient) = x + ϵ ∗ sign(∇xJ(θ,x,y))$$\n",
        "The fgsm_attack function takes three inputs: image is the original clean image (x), epsilon is the pixel-wise perturbation amount (ϵ), and data_grad is gradient of the loss w.r.t the input image.\n",
        "\n",
        "The function then creates perturbed image as\n",
        "\n",
        "$$\\text{perturbed image} = image + epsilon * sign(gradient) = x + ϵ ∗ sign(∇xJ(θ,x,y))$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5yIFALNx9sp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # FGSM attack code\n",
        "# def fgsm_attack(image, epsilon, data_grad):\n",
        "#     # Collect the element-wise sign of the data gradient\n",
        "#     sign_data_grad = data_grad.sign()\n",
        "    \n",
        "#     # Create the perturbed image by adjusting each pixel of the input image\n",
        "#     perturbed_image = image + epsilon * sign_data_grad\n",
        "    \n",
        "#     # Adding clipping to maintain [0,1] range\n",
        "#     perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
        "    \n",
        "#     # Return the perturbed image\n",
        "#     return perturbed_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0V92vJAbx9sv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # MNIST Test dataset and dataloader declaration\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('./mnist',\n",
        "                   train=False,\n",
        "                   download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                   ])),\n",
        "    batch_size=1, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ5wvIbDx9s5",
        "colab_type": "text"
      },
      "source": [
        "Test function performs a full test step on the MNIST test set and reports a final accuracy.\n",
        "\n",
        "For each sample in the test set, the function computes the gradient of the loss w.r.t the input data (data_grad),  creates a perturbed image with fgsm_attack (perturbed_data), then checks to see if the perturbed example is adversarial. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRY5_peqx9s7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def test( model, device, test_loader, epsilon ):\n",
        "\n",
        "#     # Accuracy counter\n",
        "#     correct = 0\n",
        "#     adv_examples = []\n",
        "\n",
        "#     # Loop over all examples in test set\n",
        "#     for data, target in test_loader:\n",
        "\n",
        "#         # Send the data and label to the device\n",
        "#         data, target = data.to(device), target.to(device)\n",
        "\n",
        "#         # Set requires_grad attribute of tensor. Important for Attack\n",
        "#         data.requires_grad = True\n",
        "\n",
        "#         # Forward pass the data through the model\n",
        "#         output = model(data)\n",
        "#         init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "\n",
        "#         # If the initial prediction is wrong, dont bother attacking, just move on\n",
        "#         if init_pred.item() != target.item():\n",
        "#             continue\n",
        "\n",
        "#         # Calculate the loss - Negative Log Likehood\n",
        "#         # Loosely explained at https://medium.com/deeplearningmadeeasy/negative-log-likelihood-6bd79b55d8b6\n",
        "#         loss = F.nll_loss(output, target)\n",
        "\n",
        "#         # Zero all existing gradients\n",
        "#         model.zero_grad()\n",
        "\n",
        "#         # Calculate gradients of model in backward pass\n",
        "#         loss.backward()\n",
        "\n",
        "#         # Collect datagrad\n",
        "#         data_grad = data.grad.data\n",
        "\n",
        "#         # Call FGSM Attack\n",
        "#         perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
        "\n",
        "#         # Re-classify the perturbed image\n",
        "#         output = model(perturbed_data)\n",
        "\n",
        "#         # Check for success\n",
        "#         final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "#         if final_pred.item() == target.item():\n",
        "#             correct += 1\n",
        "#             # Special case for saving 0 epsilon examples\n",
        "#             if (epsilon == 0) and (len(adv_examples) < 5):\n",
        "#                 adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
        "#                 adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
        "#         else:\n",
        "#             # Save some adv examples for visualization later\n",
        "#             if len(adv_examples) < 5:\n",
        "#                 adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
        "#                 adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
        "\n",
        "#     # Calculate final accuracy for this epsilon\n",
        "#     final_acc = correct/float(len(test_loader))\n",
        "#     print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(test_loader), final_acc))\n",
        "\n",
        "#     # Return the accuracy and an adversarial example\n",
        "#     return final_acc, adv_examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COz8UBZxx9tE",
        "colab_type": "text"
      },
      "source": [
        "# Let's attack!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FVzquqax9tH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# accuracies = []\n",
        "# examples = []\n",
        "\n",
        "# # Epsilon 0 means no attack at all\n",
        "# epsilons = [0, .05, .1, .15, .2, .25, .3]\n",
        "\n",
        "# # Run test for each epsilon\n",
        "# for eps in epsilons:\n",
        "#     acc, ex = test(model, device, test_loader, eps)\n",
        "#     accuracies.append(acc)\n",
        "#     examples.append(ex)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9E2gH87x9tN",
        "colab_type": "text"
      },
      "source": [
        "The first result is the accuracy versus epsilon plot.\n",
        "\n",
        "As epsilon increases we expect the test accuracy to decrease and it actually does. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JynJHjARx9tP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plt.figure(figsize=(5,5))\n",
        "# plt.plot(epsilons, accuracies, \"*-\")\n",
        "# plt.yticks(np.arange(0, 1.1, step=0.1))\n",
        "# plt.xticks(np.arange(0, epsilons[-1] + 0.05, step=0.05))\n",
        "# plt.title(\"Accuracy vs Epsilon\")\n",
        "# plt.xlabel(\"Epsilon\")\n",
        "# plt.ylabel(\"Accuracy\")\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzkrQViNx9tV",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmLTL2bsx9tY",
        "colab_type": "text"
      },
      "source": [
        "There is a tradeoff between accuracy degredation and perceptibility that an attacker must consider.  \n",
        "\n",
        "Below are some examples of successful adversarial examples at each epsilon value.  \n",
        "Each row of the plot shows a different epsilon value. The first row is the ϵ=0 examples which represent the original “clean” images with no perturbation. The title of each image shows the “original classification -> adversarial classification.”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLCYtbRbx9tZ",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "tougzz4nx9tb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#    # Plot several examples of adversarial samples at each epsilon\n",
        "# cnt = 0\n",
        "# plt.figure(figsize=(8,10))\n",
        "\n",
        "# for i in range(len(epsilons)):\n",
        "#     for j in range(len(examples[i])):\n",
        "#         cnt += 1\n",
        "#         plt.subplot(len(epsilons),len(examples[0]),cnt)\n",
        "#         plt.xticks([], [])\n",
        "#         plt.yticks([], [])\n",
        "#         if j == 0:\n",
        "#             plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n",
        "#         orig, adv, ex = examples[i][j]\n",
        "#         plt.title(\"True {} -> Adv {}\".format(orig, adv))\n",
        "#         plt.imshow(ex, cmap=\"gray\")\n",
        "        \n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH672P_Xx9th",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUUIA0Fjx9tj",
        "colab_type": "text"
      },
      "source": [
        "Take a closer look at how we calculated gradient. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "up5PKUgFx9tk",
        "colab_type": "code",
        "outputId": "5332d7b3-78f2-4635-ea3c-9687e43978b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "epsilon = epsilons[2]\n",
        "for data, target in test_loader:  \n",
        "    data, target = data.to(device), target.to(device)\n",
        "    data.requires_grad = True\n",
        "    \n",
        "    output = model(data)\n",
        "    loss = F.nll_loss(output, target)\n",
        "   \n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    data_grad = data.grad.data\n",
        "    perturbed_data = fgsm_attack(data, epsilon, data_grad)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-db8acc88c041>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'epsilons' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2KKi7mHx9tr",
        "colab_type": "text"
      },
      "source": [
        "The second parameter to the loss function is a true label of the current image. After that gradient is used for perturbation generation.\n",
        "<br>\n",
        "<br>\n",
        "Remember that gradient is obtained for the whole image\n",
        "pixels, namely gradient shape is equal to the image shape.  \n",
        "In our case it's (28, 28, 1). So **each pixel gets own perturbation**.\n",
        "\n",
        "Now recall how we apply gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtYWY3RNx9tu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sign_data_grad = data_grad.sign()\n",
        "# perturbed_image = img + epsilon * sign_data_grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDiD-QjVx9t0",
        "colab_type": "text"
      },
      "source": [
        "We add the scaled gradient to the original image. The result of this action is misclassification, but we don't know what label gain higher confidence. This is a case of *untargeted misclassification*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wNClv0dx9t2",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQxKHzBCx9t4",
        "colab_type": "text"
      },
      "source": [
        "*Targeted misclassification* implies that you add perturbation and model predicts one of the possible labels but not the correct one. For achieving this you should take gradient with respect to you selected label.\n",
        "\n",
        "But it's not enough. Also, you should subtract scaled gradient instead of adding. Doing this we actually perform gradient descent on the loss function surface in direction to the target class. Remember that we want **to minimize** loss function with respect to **selected class**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCdC_WzNx9t5",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01iL6zCix9t7",
        "colab_type": "text"
      },
      "source": [
        "# PRACTICUM TASK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdHizwGox9t-",
        "colab_type": "text"
      },
      "source": [
        "1 - For each class select 10 images not from this class. Perform attack to move selected images to this class. for each class select image with highest confidence.\n",
        "    \n",
        "Best sample is the one with higher confidence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsh8DOUXx9uA",
        "colab_type": "text"
      },
      "source": [
        "2 - Try different epsilons for one selected class and collect the number of iterations required to achieve success"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbeNTwJrx9uD",
        "colab_type": "text"
      },
      "source": [
        "3* OPTIONAL - make attacks using a model trained on Cifar10 obtained from the previous task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6TRdZVPhGvv",
        "colab_type": "text"
      },
      "source": [
        "# Let's rock!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHacWdobhczK",
        "colab_type": "text"
      },
      "source": [
        "importing foolbox and torch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ5iTmBRC8gR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9Y54DyTCSXa",
        "colab_type": "text"
      },
      "source": [
        "If it's necessary - install foolbox"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSHzsUxhCYsd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "b3b537d4-fb0b-4ad6-c0ca-b9f4c74a2922"
      },
      "source": [
        "pip install foolbox"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting foolbox\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/3f/cee46491ba9546d8c7bf14e18a4ecbdae411ca3d2e2ccdb227aad6de1782/foolbox-2.3.0.tar.gz (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from foolbox) (1.17.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from foolbox) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from foolbox) (42.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from foolbox) (2.21.0)\n",
            "Collecting GitPython\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/8c/4543981439d23c4ff65b2e62dddd767ebc84a8e664a9b67e840d1e2730d3/GitPython-3.0.5-py3-none-any.whl (455kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 49.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->foolbox) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->foolbox) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->foolbox) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->foolbox) (2019.11.28)\n",
            "Collecting gitdb2>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/6c/99296f89bad2ef85626e1df9f677acbee8885bb043ad82ad3ed4746d2325/gitdb2-2.0.6-py2.py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 13.5MB/s \n",
            "\u001b[?25hCollecting smmap2>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/55/d2/866d45e3a121ee15a1dc013824d58072fd5c7799c9c34d01378eb262ca8f/smmap2-2.0.5-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: foolbox\n",
            "  Building wheel for foolbox (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for foolbox: filename=foolbox-2.3.0-cp36-none-any.whl size=1926239 sha256=e5ef0762de2d81045df64a45580673f92736601c71ce0999e848c7fb1de6bd16\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/e4/a4/b6a9e61a9729c442383d774328091f69d9235268401a1c9524\n",
            "Successfully built foolbox\n",
            "Installing collected packages: smmap2, gitdb2, GitPython, foolbox\n",
            "Successfully installed GitPython-3.0.5 foolbox-2.3.0 gitdb2-2.0.6 smmap2-2.0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6uiJeG_x9uI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import foolbox\n",
        "from foolbox.models import PyTorchModel\n",
        "from foolbox.attacks import L2BasicIterativeAttack, FGSM\n",
        "from foolbox.criteria import Misclassification, ConfidentMisclassification, TargetClassProbability"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqbhZrHKhcM7",
        "colab_type": "text"
      },
      "source": [
        "Loader and criterion. We have chosen class of digit 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qHKO617hrls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('./mnist', \n",
        "                   download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                   ])),\n",
        "    batch_size=128, shuffle=True)\n",
        "\n",
        "criterion = TargetClassProbability(2, 0.90)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZTkONoNEs2o",
        "colab_type": "text"
      },
      "source": [
        "Visualization if it is necessary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5QWxfkwDWAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # visualize data\n",
        "# fig=plt.figure(figsize=(16, 8))\n",
        "# data, label = next(iter(loader))\n",
        "\n",
        "# for i in range(1, 51):\n",
        "#     img = data[i][0]\n",
        "#     fig.add_subplot(5, 10, i)\n",
        "#     plt.imshow(img, cmap='gray', vmin=0, vmax=1)\n",
        "# plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etr9DZBWRjL3",
        "colab_type": "text"
      },
      "source": [
        "Creating a dictionary, which has keys = digits and values are ten pictures, which contains these digits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhvZ6Bx_HuSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "digit_images = defaultdict(list)\n",
        "data, label = next(iter(loader))\n",
        "# label = np.array(lebel)\n",
        "\n",
        "for digit in range(10):\n",
        "  i = 0\n",
        "  while (len(digit_images[digit])) != 10 and i < len(data):\n",
        "    if label[i] == digit:\n",
        "      digit_images[digit].append(data[i][0])\n",
        "    i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD69614RnCUp",
        "colab_type": "text"
      },
      "source": [
        "Functions for adversarials"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaUhRWYBqOrh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval()\n",
        "fb_model = PyTorchModel(\n",
        "    model, \n",
        "    bounds=(-4, +4), \n",
        "    num_classes=10,\n",
        "    channel_axis=1,\n",
        ")\n",
        " \n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        # resize image to 299 pixels in width and hight\n",
        "        transforms.Resize((299,299)),\n",
        "        \n",
        "        # transorm \"Image\" object to \"tensor\" onject. Used when working with PIL.Image\n",
        "        transforms.ToTensor(),\n",
        "        \n",
        "        # Normalize image per chanel\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqmJsIeXR8NY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "def restore_image(image):\n",
        "    # Move chanel axis [3, 299, 299] -> [299, 299, 3]\n",
        "    new_image = np.rollaxis(np.rollaxis((image), 2), 2)\n",
        "    \n",
        "    # Multiply by std and add mean\n",
        "    new_image = (new_image * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]\n",
        "    \n",
        "    # Move from range 0-1 to the range 0-255\n",
        "    new_image = new_image * 255\n",
        "    \n",
        "    # Make sure to remove all values that lower that 0 or higher than 255\n",
        "    # as it not valid images\n",
        "    new_image = np.clip(new_image, 0, 255)\n",
        "    \n",
        "    # Put image to the \"byte\" format. \n",
        "    # That`s required by PIL.Image to be abble to restore image from numpy array\n",
        "    new_image = new_image.astype(np.uint8)\n",
        "    return new_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk-S8cWdR8N2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_adversarial(foolbox_model, attack, selected_criterion, image):\n",
        "    attack = attack(\n",
        "        model=foolbox_model,\n",
        "        criterion=selected_criterion\n",
        "    )\n",
        "    normalized_image = image.unsqueeze(0).to(device)\n",
        "    ########### I added  another unsqueeze, because it caused an error ########\n",
        "    normalized_image = normalized_image.unsqueeze(0).to(device)\n",
        "    prediction = model(normalized_image)[0]\n",
        "    predicted_class = prediction.argmax(-1).cpu().numpy()\n",
        "    \n",
        "    normalazed_input_numpy = normalized_image.cpu().numpy()\n",
        "    predicted_labels = np.array([int(predicted_class)])\n",
        "\n",
        "    new_images = attack(normalazed_input_numpy, labels=predicted_labels)\n",
        "    \n",
        "    restored_numpy_array = restore_image(new_images[0])\n",
        "    restored_image = Image.fromarray(restored_numpy_array)\n",
        "    return restored_image\n",
        "\n",
        "def print_prediction(image):\n",
        "  normalized_image = transform(image).unsqueeze(0).to(device)\n",
        "  # print(normalized_image.size())\n",
        "  # normalized_image = normalized_image.unsqueeze(0).to(device)\n",
        "  prediction = model(normalized_image)[0]\n",
        "  predicted_class = prediction.argmax(-1).cpu().numpy()\n",
        "  print(f\"Predicted class {int(predicted_class)} : {imagenet_labels[int(predicted_class)]}\")\n",
        "  print(f\"Probability: {torch.softmax(prediction, -1)[predicted_class]:.3f}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0Qab3ZS3w9f",
        "colab_type": "text"
      },
      "source": [
        "#Checking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQs1Xmob3zh0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "e5c1a2eb-4fcf-4258-eb40-3eaca599a246"
      },
      "source": [
        "adv_image = generate_adversarial(\n",
        "    fb_model, \n",
        "    L2BasicIterativeAttack,\n",
        "    TargetClassProbability(2, 0.95), \n",
        "    digit_images[7][0]\n",
        ")\n",
        "print_prediction(adv_image)\n",
        "adv_image"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-3ac573d48c38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdigit_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madv_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0madv_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-69-5c33554b74e9>\u001b[0m in \u001b[0;36mprint_prediction\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;31m# print(normalized_image.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;31m# normalized_image = normalized_image.unsqueeze(0).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m   \u001b[0mpredicted_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Predicted class {int(predicted_class)} : {imagenet_labels[int(predicted_class)]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-315450923831>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m320\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size 10 1 5 5, expected input[1, 3, 299, 299] to have 1 channels, but got 3 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJoefjSjVeEm",
        "colab_type": "text"
      },
      "source": [
        "Fuctions for generating adversarials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9s_cFS7nVwLq",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "adOyDklyUFU8",
        "colab": {}
      },
      "source": [
        "# normalized_image = digit_images[7][1].unsqueeze(0).to(device)\n",
        "# normalized_image = normalized_image.unsqueeze(0).to(device)\n",
        "# normalized_image.size()\n",
        "# prediction = model(normalized_image)[0]\n",
        "# predicted_class = prediction.argmax(-1).cpu().numpy()\n",
        "# print(f\"Predicted class {int(predicted_class)} : {'7'}\")\n",
        "# print(f\"Probability: {torch.softmax(prediction, -1)[predicted_class]:.4f}\")\n",
        "\n",
        "# # First we need to move our input from torch to numpy array, as it required by FoolBox\n",
        "# normalazed_input_numpy = normalized_image.cpu().numpy()\n",
        "\n",
        "# # Than we need to do a little modification to the predicted class\n",
        "# # Namely move it to array of shape [batch_size], in our case [1]\n",
        "# predicted_labels = np.array([int(predicted_class)])\n",
        "# # And finaly we can generate new image provided starting point and true label\n",
        "# new_images = attack(normalazed_input_numpy, labels=predicted_labels)\n",
        "# new_images = attack(new_images, labels=predicted_labels)\n",
        "# prediction = model(new_images)[0]\n",
        "# predicted_class = prediction.argmax(-1).cpu().numpy()\n",
        "# print(f\"Predicted class {int(predicted_class)} : {'7'}\")\n",
        "# print(f\"Probability: {torch.softmax(prediction, -1)[predicted_class]:.4f}\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}